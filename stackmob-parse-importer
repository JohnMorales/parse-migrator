#!/usr/bin/env python
import client
import argparse
import cmd
import signal
import sys
import json
import os
import textwrap
import json
import dateutil.parser
import time
from util import hasattrs, read_api_key_and_secret_or_die, api_key_name, api_secret_name, path_name

parser = argparse.ArgumentParser(prog = "stackmob-parse-importer", description='move data from Parse to StackMob')
parser.add_argument("--%s"%("api_key"), help="the API key for your app", metavar="api-key", required=False, dest=api_key_name)
parser.add_argument("--%s"%("api_secret"), help="the API secret for your app", metavar="api-secret", required=False, dest=api_secret_name)
parser.add_argument("--%s"%("path"), help="the path to you unzipped parse export", metavar="path", required=False, dest=path_name)
parser.add_argument("--verbose", help="output details on requests made to stackmob", dest="verbose", type=bool)


def signal_handler(signal, frame):
	print ""
	sys.exit(0)

def print_resp(resp):
	print textwrap.dedent("""
	response code: %d
	response headers: %s
	response body: %s
	"""%(resp.status_code, resp.headers, resp.text))


def iso_date_to_unix(date):
	return int(dateutil.parser.parse(date).strftime("%s") + "000")

def rename_and_reformat_date(obj, old, new):
	date = obj.pop(old, None)
	object[new] = iso_date_to_unix(date)


if __name__=="__main__":
	signal.signal(signal.SIGINT, signal_handler)
	args = parser.parse_args()
	api_key, api_secret, path = read_api_key_and_secret_or_die(vars(args))

	debug_level = 0
	if(args.verbose):	
		print "using verbose mode"
		debug_level = 1
	
	api_client = client.DatastoreClient(api_key, api_secret, debug_level=debug_level)

	exported_files = os.listdir(path)

	# Relations, stored separately from schemas
	joins = filter(lambda x: x.startswith("_Join"), exported_files)

	# Regular schemas
	schemas = filter(lambda x: x == "_User.json" or not x.startswith("_"), exported_files)


	for schema in schemas:
		schema_name = schema[:-5]
		
		# convert to the default user schema
		if schema_name == "_User":
			schema_name = "user"

		schema_id = schema_name + "_id"
		schema_joins = filter(lambda x: x.endswith(schema), joins)


		file = open(path + "/" + schema, "r")
		parse_json = json.loads(file.read())
		objects = parse_json["results"]
		for object in objects:
			# rename the id
			id = object.pop("objectId", None)
			object[schema_id] = id

			# reformat and rename the timestamps
			rename_and_reformat_date(object, "createdAt", "createddate")
			rename_and_reformat_date(object, "updatedAt", "lastmoddate")


			# convert over complex datatypes
			for field in object.keys():
				value = object[field]
				if isinstance(value, dict):
					subobject = value
					if "__type" in subobject:
						type = subobject["__type"]
						if type == "File":
							object.pop(field, None)
							print "Skipping field %s. Files are not yet supported"%(field)
						elif type == "GeoPoint":
							object.pop(field, None)
							lat = subobject["latitude"]
							lon = subobject["longitude"]
							# just differe lat/lon names
							object[field] = {"lat": lat, "lon": lon}
						elif type == "Date":
							object.pop(field, None)
							date_string = subobject["iso"]
							object[field] = iso_date_to_unix(date_string) 
						elif type == "Pointer":
							object.pop(field, None)
							print "Skipping field %s. Pointers are not yet supported"%(field)
						else:
							object.pop(field, None)
							print "Skipping field %s. Subobjects are not yet supported"%(field)

			# save the data twice so the timestamp fields get set. This is
			# a bug that probably only would be noticed in a migration
			api_client.post(schema_name, object, "")
			print api_client.post(schema_name, object, "")

#for join in joins:
		# we can get the name of the field and the schema from the join filename
#parts = join.split(".")[0].split(":")
#field = parts[1]
#owner_schema = parts[2]

		# however, there's no indication what schema it goes to. Parse seems to have
		# globally unique ids, we do not. Figuring out the related schema is a bit hacky






